<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Tianjie Zhang</title>

    <meta name="author" content="Tianjie Zhang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/C.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:70%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Tianjie Zhang&nbsp;&nbsp;&nbsp;<span style="font-size: 0.9em;">张天杰</span>
                </p>
                <p style="text-align: center; font-size: 0.8em;">
                  <code>tianjiez [at] cs [dot] cmu [dot] edu</code>
                </p>
                <p>
    I'm a master's student in the <a href="https://www.ml.cmu.edu/">Machine Learning Department</a> at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>. I received my bachelor's degree <a href="http://ckc.zju.edu.cn/ckcen/">with honors</a> in Computer Science and Technology from <a href="https://www.zju.edu.cn/english/">Zhejiang University</a>.
                <p>
    My interest lies in large language models and language agents. Previously, I was a research associate member at <a href="https://sail.sea.com/">Sea AI Lab</a>, fortunate to be advised by <a href="https://longxudou.github.io/">Longxu Dou</a> and <a href="https://linmin.me/">Min Lin</a>. I worked on agents that interact with computers via terminal environments.
                </p>
                <p>
    From January 2024 to July 2024, I enjoyed a wonderful time at <a href="https://keg.cs.tsinghua.edu.cn/home/">THUKEG</a>, working with Professor <a href="https://keg.cs.tsinghua.edu.cn/jietang/">Jie Tang</a> and <a href="https://keg.cs.tsinghua.edu.cn/yuxiao/">Yuxiao Dong</a> on vision language model agents. During my undergraduate studies, I was also fortunate to be advised by Professor <a href="http://yangy.org/">Yang Yang</a>.
                </p>
                <p>
    <font color="red"><strong>I'm seeking Machine Learning Engineer internships / Research Engineer internships for the summer of 2026.</strong></font>
                </p>
                <p style="text-align:center">
                  <a href="data/Tianjie_Zhang_Resume.pdf">Resume</a>
                  &nbsp;&nbsp;/&nbsp;&nbsp;
                  <a href="https://www.linkedin.com/in/tianjiez/">LinkedIn</a>
                  &nbsp;&nbsp;/&nbsp;&nbsp;
                  <a href="https://github.com/mistyreed63849/">Github</a>
                  &nbsp;&nbsp;/&nbsp;&nbsp;
                  <a href="https://scholar.google.com/citations?user=rixbAQoAAAAJ&hl=en">Google Scholar</a>
                  &nbsp;&nbsp;/&nbsp;&nbsp;
                  <a href="https://x.com/tianjie_zhang1">X</a>
                </p>
              </td>
              <td style="padding:0.8%;width:30%;max-width:30%">
                <a href="images/Tianjie_Zhang.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 40px;" alt="profile photo" src="images/Tianjie_Zhang.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Experience</h2>
                      <p>
                          From March 2025 to August 2025, I was a research associate member at <a href="https://sail.sea.com/">Sea AI Lab</a>, advised by <a href="https://longxudou.github.io/">Longxu Dou</a> and <a href="https://linmin.me/">Min Lin</a>. We work on large language model agents that interact with computers through terminal interfaces, with my primary focus on agent scaffolding, training data collection and benchmarking.
                      </p>
                      <p>
                          From January 2024 to July 2024, I was a research intern at <a href="https://keg.cs.tsinghua.edu.cn/home/">THUKEG</a>, collaborating closely with <a href="https://github.com/xiao9905">Xiao Liu</a> and <a href="https://entslscheia.github.io/">Yu Gu</a>, under the supervision of Professor <a href="https://keg.cs.tsinghua.edu.cn/jietang/">Jie Tang</a> and <a href="https://keg.cs.tsinghua.edu.cn/yuxiao/">Yuxiao Dong</a>. We released <a href="https://github.com/THUDM/VisualAgentBench">VisualAgentBench</a>, a systematic benchmark to evaluate and develop vision language models as visual foundation agents. It has received over 50 citations and garnered <a href="https://github.com/THUDM/VisualAgentBench"><img alt="GitHub stars" src="https://img.shields.io/github/stars/THUDM/VisualAgentBench?style=social" class="github" style="height: 15px;"></a>.
                      </p>
                      <p>
                          Prior to that, I worked on enhancing the graph reasoning ability of large language models with <a href="https://scholar.google.com/citations?hl=en&user=m9a1Uu0AAAAJ">Ziwei Chai</a>, under the supervision of Professor <a href="http://yangy.org/">Yang Yang</a>. We proposed <a href="https://arxiv.org/abs/2310.05845">GraphLLM</a>, which enables large language models to solve fundamental graph reasoning tasks with near-perfect accuracy. It has received over 140 citations and garnered <a href="https://github.com/mistyreed63849/Graph-LLM"><img alt="GitHub stars" src="https://img.shields.io/github/stars/mistyreed63849/Graph-LLM?style=social" class="github" style="height: 15px;"></a>.
                      </p>
              </td>
            </tr>
            <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Publications / Preprints</h2>
                <p style="margin-top: 10px; margin-bottom: -12px;">
                  * denotes equal contribution.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

    <tr>
      <!-- <td style="padding:16px;width:25%;vertical-align:middle">
        <div style="height:4em;"></div>
        <div class="one">
          <img src='images/VisualAgentBench.png' width=100%>
        </div>
      </td> -->
      <td style="padding:16px;width:100%;vertical-align:middle">
        <a href="https://github.com/THUDM/VisualAgentBench">
          <span class="papertitle">VisualAgentBench: Towards Large Multimodal Models as Visual Foundation Agents</span>
        </a>
        <br>
        <a href="https://github.com/xiao9905">Xiao Liu</a>*,
        <strong>Tianjie Zhang</strong>*,
        <a href="https://entslscheia.github.io/">Yu Gu</a>*,
        <a href="https://scholar.google.com/citations?user=FD3tIC4AAAAJ&hl=en">Iat Long Iong</a>,
        <a href="https://scholar.google.com/citations?user=fPvbfBUAAAAJ&hl=en">Yifan Xu</a>,
        <a href="https://openreview.net/profile?id=~Song_XiXuan1">Xixuan Song</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=Z-SskNIAAAAJ">Shudan Zhang</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=cuFVHjwAAAAJ">Hanyu Lai</a>,
        Xinyi Liu,
        <a href="https://scholar.google.com/citations?user=2sVac3EAAAAJ&hl=en">Hanlin Zhao</a>,
        <a href="https://openreview.net/profile?id=~Jiadai_Sun2">Jiadai Sun</a>,
        <a href="https://openreview.net/profile?id=~Xinyue_Yang2">Xinyue Yang</a>,
        <a href="https://openreview.net/profile?id=~Yu_Yang26">Yu Yang</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=kfZ5xAIAAAAJ">Zehan Qi</a>,
        <a href="https://openreview.net/profile?id=~Shuntian_Yao1">Shuntian Yao</a>,
        <a href="https://openreview.net/profile?id=~Xueqiao_Sun1">Xueqiao Sun</a>,
        <a href="https://openreview.net/profile?id=~Siyi_Cheng1">Siyi Cheng</a>,
        <a href="https://scholar.google.com/citations?user=54wdDqcAAAAJ&hl=en">Qinkai Zheng</a>,
        <a href="https://scholar.google.com/citations?user=DnYC9yoAAAAJ&hl=en">Hao Yu</a>,
        <a href="https://scholar.google.com/citations?user=pGcJcagAAAAJ&hl=en">Hanchen Zhang</a>,
        <a href="https://wenyihong.github.io/">Wenyi Hong</a>,
        <a href="https://scholar.google.com/citations?user=Va50YzkAAAAJ&hl=en">Ming Ding</a>,
        <a href="https://openreview.net/profile?id=~Lihang_Pan1">Lihang Pan</a>,
        <a href="https://xiaotao2.web.illinois.edu/">Xiaotao Gu</a>,
        <a href="https://scholar.google.com/citations?user=STftvjoAAAAJ&hl=en">Aohan Zeng</a>,
        <a href="https://zxdu.xyz/">Zhengxiao Du</a>,
        <a href="https://chanh.ee/">Chan Hee Song</a>,
        <a href="https://ysu1989.github.io/">Yu Su</a>,
        <a href="https://keg.cs.tsinghua.edu.cn/yuxiao/">Yuxiao Dong</a>,
        <a href="https://keg.cs.tsinghua.edu.cn/jietang/">Jie Tang</a>
    
        <br>
        <em>ICLR</em>, 2025
        <br>
        <a href="https://openreview.net/pdf?id=2snKOc7TVp">Paper</a>
        &nbsp; / &nbsp;
        <a href="https://github.com/THUDM/VisualAgentBench">Code & Data</a>
        &nbsp; / &nbsp;
        <a href="images/VAB_poster.png">Poster</a>
        <div style="height:0.15em;"></div>
          A systematic benchmark for evaluating and developing large multimodal models as visual foundation agents across Embodied, GUI, and Visual Design scenarios.
      </td>
    </tr>

    <tr>
      <!-- <td style="padding:16px;width:20%;vertical-align:middle">
        <div style="height:1em;"></div>
        <div class="one">
          <img src='images/GraphLLM.png' width=100%>
        </div>
      </td> -->
      <td style="padding:16px;width:100%;vertical-align:middle">
        <a href="https://arxiv.org/abs/2310.05845">
          <span class="papertitle">GraphLLM: Boosting Graph Reasoning Ability of Large Language Model</span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?hl=en&user=m9a1Uu0AAAAJ">Ziwei Chai</a>*,
        <strong>Tianjie Zhang</strong>*,
        <a href="https://openreview.net/profile?id=~Liang_Wu8">Liang Wu</a>,
        <a href="https://scholar.google.com/citations?user=gFBnb-AAAAAJ&hl=en">Kaiqiao Han</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=AixM5hcAAAAJ">Xiaohai Hu</a>,
        <a href="https://scholar.google.com/citations?hl=en&user=JFLCWNQAAAAJ">Xuanwen Huang</a>,
        <a href="http://yangy.org/">Yang Yang</a>
    
        <br>
        <em>Under review</em>, 2025
        <br>
        <a href="https://arxiv.org/pdf/2310.05845">Paper</a>
        &nbsp; / &nbsp;
        <a href="https://github.com/mistyreed63849/Graph-LLM">Code</a>
        <div style="height:0.15em;"></div>
          Enabling large language models to proficiently interpret and reason on graph data.
      </td>
    </tr>

    <tr>
      <!-- <td style="padding:16px;width:20%;vertical-align:middle">
        <div class="one">
          <img src='images/ExpertTokenRouting.png' width=100%>
        </div>
      </td> -->
      <td style="padding:16px;width:100%;vertical-align:middle">
        <a href="https://aclanthology.org/2024.acl-long.614/">
          <span class="papertitle">An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing</span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?hl=en&user=m9a1Uu0AAAAJ">Ziwei Chai</a>,
        <a href="https://guoyin.wang/">Guoyin Wang</a>,
        <a href="https://openreview.net/profile?id=~Jing_Su2">Jing Su</a>,
        <strong>Tianjie Zhang</strong>,
        <a href="https://scholar.google.com/citations?hl=en&user=JFLCWNQAAAAJ">Xuanwen Huang</a>,
        <a href="https://scholar.google.com/citations?user=Fww9LGsAAAAJ&hl=en">Xuwu Wang</a>,
        <a href="https://jingjingxu.com/">Jingjing Xu</a>,
        <a href="https://scholar.google.com/citations?user=B1EhbCsAAAAJ&hl=en">Jianbo Yuan</a>,
        <a href="https://www4.comp.polyu.edu.hk/~hongxyang/">Hongxia Yang</a>,
        <a href="https://person.zju.edu.cn/en/wufei">Fei Wu</a>,
        <a href="http://yangy.org/">Yang Yang</a>
    
        <br>
        <em>ACL</em>, 2024
        <br>
        <a href="https://aclanthology.org/2024.acl-long.614.pdf">Paper</a>
        &nbsp; / &nbsp;
        <a href="https://github.com/Illyasville/ExpertTokenRouting">Code</a>
        <div style="height:0.15em;"></div>
          A unified framework that facilitates the seamless integration of multiple expert LLMs.
      </td>
    </tr>

          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  <span id="busuanzi_container_page_pv">
                    <span id="busuanzi_value_page_pv"></span>
                  </span>
                  &nbsp;&nbsp;&nbsp;
                  <a href="https://jonbarron.info/">Credits</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
